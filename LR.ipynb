{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec3f390a",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c6b6bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90897c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "\n",
    "data=pd.read_csv('preprocessed_UK_Accidents_2009_updated.csv',index_col='accident_index')\n",
    "data=data.drop('seasons_ranges',axis=1)\n",
    "x = data.drop('accident_severity', axis=1) \n",
    "y = data['accident_severity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ccd5d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalling data\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94fd3343",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA\n",
    "priciple_component_analayzer = PCA(n_components=15) # based on domain knowledge from feature extraction notebook\n",
    "x_pca = priciple_component_analayzer.fit_transform(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1c925f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    134714\n",
       "2     21475\n",
       "3      2003\n",
       "Name: accident_severity, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Imbalance\n",
    "class_counts = y.value_counts()\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28c40694",
   "metadata": {},
   "outputs": [],
   "source": [
    "## As shown above the data suffer from sever data imbalance\n",
    "## Solutions:\n",
    "## 1-class weights (not available in KNN)\n",
    "## 2- Resampling (random resampling will be used in our case )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c39e4103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8519741490224076\n",
      "Testing accuracy: 0.8493826119937629\n",
      "Training F1 score: 0.8519741490224076\n",
      "Testing F1 score: 0.8493826119937629\n",
      "Training precision: 0.8519741490224076\n",
      "Testing precision: 0.8493826119937629\n",
      "Training recall: 0.8519741490224076\n",
      "Testing recall: 0.8493826119937629\n"
     ]
    }
   ],
   "source": [
    "## base model before class weights and resampling\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Create a logistic regression model\n",
    "log_reg = LogisticRegression(max_iter=10000,solver='liblinear',C=0.02,penalty='l1')\n",
    "\n",
    "# Train the model on the training data\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of the model on the training and testing data\n",
    "y_train_pred = log_reg.predict(X_train)\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "train_f1 = f1_score(y_train, y_train_pred, average='micro')\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='micro')\n",
    "\n",
    "train_precision = precision_score(y_train, y_train_pred, average='micro')\n",
    "test_precision = precision_score(y_test, y_test_pred, average='micro')\n",
    "\n",
    "train_recall = recall_score(y_train, y_train_pred, average='micro')\n",
    "test_recall = recall_score(y_test, y_test_pred, average='micro')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Training accuracy:\", train_accuracy)\n",
    "print(\"Testing accuracy:\", test_accuracy)\n",
    "\n",
    "print(\"Training F1 score:\", train_f1)\n",
    "print(\"Testing F1 score:\", test_f1)\n",
    "\n",
    "print(\"Training precision:\", train_precision)\n",
    "print(\"Testing precision:\", test_precision)\n",
    "\n",
    "print(\"Training recall:\", train_recall)\n",
    "print(\"Testing recall:\", test_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03379247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.4867867955286446\n",
      "Testing accuracy: 0.4890633763320247\n",
      "Training F1 score: 0.4867867955286446\n",
      "Testing F1 score: 0.4890633763320247\n",
      "Training precision: 0.4867867955286446\n",
      "Testing precision: 0.4890633763320247\n",
      "Training recall: 0.4867867955286446\n",
      "Testing recall: 0.4890633763320247\n"
     ]
    }
   ],
   "source": [
    "## base model after resampling\n",
    "oversampler = RandomOverSampler(random_state=10)\n",
    "x_resampled, y_resampled = oversampler.fit_resample(x_pca, y)\n",
    "x_resampled=pd.DataFrame(x_resampled)\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_resampled, y_resampled, test_size=0.15, random_state=42)\n",
    "\n",
    "# Create a logistic regression model\n",
    "log_reg = LogisticRegression(max_iter=10000,solver='liblinear',C=0.02,penalty='l1')\n",
    "\n",
    "# Train the model on the training data\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of the model on the training and testing data\n",
    "y_train_pred = log_reg.predict(X_train)\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "train_f1 = f1_score(y_train, y_train_pred, average='micro')\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='micro')\n",
    "\n",
    "train_precision = precision_score(y_train, y_train_pred, average='micro')\n",
    "test_precision = precision_score(y_test, y_test_pred, average='micro')\n",
    "\n",
    "train_recall = recall_score(y_train, y_train_pred, average='micro')\n",
    "test_recall = recall_score(y_test, y_test_pred, average='micro')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Training accuracy:\", train_accuracy)\n",
    "print(\"Testing accuracy:\", test_accuracy)\n",
    "\n",
    "print(\"Training F1 score:\", train_f1)\n",
    "print(\"Testing F1 score:\", test_f1)\n",
    "\n",
    "print(\"Training precision:\", train_precision)\n",
    "print(\"Testing precision:\", test_precision)\n",
    "\n",
    "print(\"Training recall:\", train_recall)\n",
    "print(\"Testing recall:\", test_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3164f6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8258256918260042\n",
      "Testing accuracy: 0.8238442412238189\n",
      "Training F1 score: 0.8258256918260042\n",
      "Testing F1 score: 0.8238442412238189\n",
      "Training precision: 0.8258256918260042\n",
      "Testing precision: 0.8238442412238189\n",
      "Training recall: 0.8258256918260042\n",
      "Testing recall: 0.8238442412238189\n"
     ]
    }
   ],
   "source": [
    "## base model after class weights\n",
    "class_labels = np.unique(y)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=y)\n",
    "class_weight_value = {class_labels[i]: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_pca, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Create a logistic regression model\n",
    "log_reg = LogisticRegression(max_iter=10000,class_weight=class_weight_value,solver='liblinear',C=0.02,penalty='l1')\n",
    "# Train the model on the training data\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of the model on the training and testing data\n",
    "y_train_pred = log_reg.predict(X_train)\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "train_f1 = f1_score(y_train, y_train_pred, average='micro')\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='micro')\n",
    "\n",
    "train_precision = precision_score(y_train, y_train_pred, average='micro')\n",
    "test_precision = precision_score(y_test, y_test_pred, average='micro')\n",
    "\n",
    "train_recall = recall_score(y_train, y_train_pred, average='micro')\n",
    "test_recall = recall_score(y_test, y_test_pred, average='micro')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Training accuracy:\", train_accuracy)\n",
    "print(\"Testing accuracy:\", test_accuracy)\n",
    "\n",
    "print(\"Training F1 score:\", train_f1)\n",
    "print(\"Testing F1 score:\", test_f1)\n",
    "\n",
    "print(\"Training precision:\", train_precision)\n",
    "print(\"Testing precision:\", test_precision)\n",
    "\n",
    "print(\"Training recall:\", train_recall)\n",
    "print(\"Testing recall:\", test_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01d57d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## As the results show, the resampling is not a good technique to use in our case. Although the model without any\n",
    "## data imbalance technique gave the highest scores, class weights will be used as a safe practice (difference is negligible\n",
    "## and class weights will make model evaluation more fair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35409b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=10,stratify=y)## stratify=y used to have the same ration of classes in each split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23498154",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter tuning using search grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94408dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## before runing the grid, a smaller grid will be used to determine the range of C that we should be using to \n",
    "## reduce the computational time of the main grid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f6111750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 7 candidates, totalling 14 fits\n",
      "[CV 1/2] END C=1e-05; accuracy: (test=0.852) f1: (test=0.852) precision: (test=0.852) recall: (test=0.852) total time=   0.8s\n",
      "[CV 2/2] END C=1e-05; accuracy: (test=0.852) f1: (test=0.852) precision: (test=0.852) recall: (test=0.852) total time=   1.0s\n",
      "[CV 1/2] END C=0.0001; accuracy: (test=0.849) f1: (test=0.849) precision: (test=0.849) recall: (test=0.849) total time=   1.3s\n",
      "[CV 2/2] END C=0.0001; accuracy: (test=0.847) f1: (test=0.847) precision: (test=0.847) recall: (test=0.847) total time=   2.4s\n",
      "[CV 1/2] END C=0.001; accuracy: (test=0.834) f1: (test=0.834) precision: (test=0.834) recall: (test=0.834) total time=  10.7s\n",
      "[CV 2/2] END C=0.001; accuracy: (test=0.832) f1: (test=0.832) precision: (test=0.832) recall: (test=0.832) total time=   8.6s\n",
      "[CV 1/2] END C=0.01; accuracy: (test=0.821) f1: (test=0.821) precision: (test=0.821) recall: (test=0.821) total time=  12.3s\n",
      "[CV 2/2] END C=0.01; accuracy: (test=0.818) f1: (test=0.818) precision: (test=0.818) recall: (test=0.818) total time=  14.3s\n",
      "[CV 1/2] END C=0.1; accuracy: (test=0.816) f1: (test=0.816) precision: (test=0.816) recall: (test=0.816) total time=  16.1s\n",
      "[CV 2/2] END C=0.1; accuracy: (test=0.815) f1: (test=0.815) precision: (test=0.815) recall: (test=0.815) total time=  21.2s\n",
      "[CV 1/2] END C=1; accuracy: (test=0.816) f1: (test=0.816) precision: (test=0.816) recall: (test=0.816) total time=  38.2s\n",
      "[CV 2/2] END C=1; accuracy: (test=0.815) f1: (test=0.815) precision: (test=0.815) recall: (test=0.815) total time= 1.0min\n",
      "[CV 1/2] END C=10; accuracy: (test=0.816) f1: (test=0.816) precision: (test=0.816) recall: (test=0.816) total time=  20.3s\n",
      "[CV 2/2] END C=10; accuracy: (test=0.815) f1: (test=0.815) precision: (test=0.815) recall: (test=0.815) total time=  20.5s\n",
      "Best hyperparameters: {'C': 1e-05}\n",
      "Best accuracy: 0.8518774674238827\n",
      "Best precision: 0.8518774674238827\n",
      "Best recall: 0.8518774674238827\n",
      "Best F1 score: 0.8518774674238827\n"
     ]
    }
   ],
   "source": [
    "# create a logistic regression model\n",
    "log_reg = LogisticRegression(max_iter=10000,class_weight=class_weight_value,solver='liblinear',penalty='l1')\n",
    "\n",
    "# define the hyperparameters to search over\n",
    "param_grid = {'C': [0.00001,0.0001,0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "# create custom scoring functions to calculate accuracy, precision, recall, and F1 score with average='macro'\n",
    "accuracy_scorer = make_scorer(accuracy_score)\n",
    "precision_scorer = make_scorer(precision_score, average='micro', zero_division=0)\n",
    "recall_scorer = make_scorer(recall_score, average='micro')\n",
    "f1_scorer = make_scorer(f1_score, average='micro')\n",
    "\n",
    "# perform grid search and obtain performance metrics for each combination of hyperparameters\n",
    "grid_search = GridSearchCV(log_reg, param_grid=param_grid, cv=StratifiedKFold(n_splits=2, random_state=10, shuffle=True), scoring={'accuracy': accuracy_scorer, 'precision': precision_scorer, 'recall': recall_scorer, 'f1': f1_scorer}, refit='precision', verbose=3)\n",
    "\n",
    "# fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print the best hyperparameters and corresponding performance metrics\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best accuracy:\", grid_search.best_score_)\n",
    "print(\"Best precision:\", grid_search.cv_results_['mean_test_precision'][grid_search.best_index_])\n",
    "print(\"Best recall:\", grid_search.cv_results_['mean_test_recall'][grid_search.best_index_])\n",
    "print(\"Best F1 score:\", grid_search.cv_results_['mean_test_f1'][grid_search.best_index_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5f4c240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## based on the results, small values of Cs are better so, we will be looping over small values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a00f3d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "[CV 1/2] END penalty=l1, solver=liblinear; accuracy: (test=0.852) f1: (test=0.852) precision: (test=0.852) recall: (test=0.852) total time=   0.8s\n",
      "[CV 2/2] END penalty=l1, solver=liblinear; accuracy: (test=0.852) f1: (test=0.852) precision: (test=0.852) recall: (test=0.852) total time=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\express\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END penalty=l1, solver=saga; accuracy: (test=0.086) f1: (test=0.086) precision: (test=0.086) recall: (test=0.086) total time=19.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\express\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END penalty=l1, solver=saga; accuracy: (test=0.136) f1: (test=0.136) precision: (test=0.136) recall: (test=0.136) total time=19.9min\n",
      "[CV 1/2] END penalty=l2, solver=liblinear; accuracy: (test=0.852) f1: (test=0.852) precision: (test=0.852) recall: (test=0.852) total time=   3.0s\n",
      "[CV 2/2] END penalty=l2, solver=liblinear; accuracy: (test=0.852) f1: (test=0.852) precision: (test=0.852) recall: (test=0.852) total time=   3.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\express\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/2] END penalty=l2, solver=saga; accuracy: (test=0.246) f1: (test=0.246) precision: (test=0.246) recall: (test=0.246) total time=17.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\express\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/2] END penalty=l2, solver=saga; accuracy: (test=0.346) f1: (test=0.346) precision: (test=0.346) recall: (test=0.346) total time=17.2min\n",
      "Best hyperparameters: {'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Best accuracy: 0.8515874255289281\n",
      "Best precision: 0.8515874255289281\n",
      "Best recall: 0.8515874255289281\n",
      "Best F1 score: 0.851587425528928\n"
     ]
    }
   ],
   "source": [
    "# create a logistic regression model\n",
    "log_reg = LogisticRegression(max_iter=10000,class_weight=class_weight_value,C=0.00000001)\n",
    "\n",
    "# define the hyperparameters to search over\n",
    "param_grid = {'penalty': ['l1', 'l2'],\n",
    "              'solver': ['liblinear', 'saga']}\n",
    "## liblinear and saga are good for multiclass data and large datasets\n",
    "\n",
    "# create custom scoring functions to calculate accuracy, precision, recall, and F1 score with average='macro'\n",
    "accuracy_scorer = make_scorer(accuracy_score)\n",
    "precision_scorer = make_scorer(precision_score, average='micro', zero_division=0)\n",
    "recall_scorer = make_scorer(recall_score, average='micro')\n",
    "f1_scorer = make_scorer(f1_score, average='micro')\n",
    "\n",
    "# perform grid search and obtain performance metrics for each combination of hyperparameters\n",
    "grid_search = GridSearchCV(log_reg, param_grid=param_grid, cv=StratifiedKFold(n_splits=2, random_state=10, shuffle=True), scoring={'accuracy': accuracy_scorer, 'precision': precision_scorer, 'recall': recall_scorer, 'f1': f1_scorer}, refit='precision', verbose=3)\n",
    "\n",
    "# fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print the best hyperparameters and corresponding performance metrics\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best accuracy:\", grid_search.best_score_)\n",
    "print(\"Best precision:\", grid_search.cv_results_['mean_test_precision'][grid_search.best_index_])\n",
    "print(\"Best recall:\", grid_search.cv_results_['mean_test_recall'][grid_search.best_index_])\n",
    "print(\"Best F1 score:\", grid_search.cv_results_['mean_test_f1'][grid_search.best_index_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48430305",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8519741490224076\n",
      "Testing accuracy: 0.8493826119937629\n",
      "Training F1 score: 0.8519741490224076\n",
      "Testing F1 score: 0.8493826119937629\n",
      "Training precision: 0.8519741490224076\n",
      "Testing precision: 0.8493826119937629\n",
      "Training recall: 0.8519741490224076\n",
      "Testing recall: 0.8493826119937629\n"
     ]
    }
   ],
   "source": [
    "## base model after class weights\n",
    "class_labels = np.unique(y)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=y)\n",
    "class_weight_value = {class_labels[i]: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_pca, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Create a logistic regression model\n",
    "log_reg = LogisticRegression(max_iter=10000,class_weight=class_weight_value,solver='liblinear',C=0.00000001,penalty='l1')\n",
    "# Train the model on the training data\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of the model on the training and testing data\n",
    "y_train_pred = log_reg.predict(X_train)\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "train_f1 = f1_score(y_train, y_train_pred, average='micro')\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='micro')\n",
    "\n",
    "train_precision = precision_score(y_train, y_train_pred, average='micro')\n",
    "test_precision = precision_score(y_test, y_test_pred, average='micro')\n",
    "\n",
    "train_recall = recall_score(y_train, y_train_pred, average='micro')\n",
    "test_recall = recall_score(y_test, y_test_pred, average='micro')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Training accuracy:\", train_accuracy)\n",
    "print(\"Testing accuracy:\", test_accuracy)\n",
    "\n",
    "print(\"Training F1 score:\", train_f1)\n",
    "print(\"Testing F1 score:\", test_f1)\n",
    "\n",
    "print(\"Training precision:\", train_precision)\n",
    "print(\"Testing precision:\", test_precision)\n",
    "\n",
    "print(\"Training recall:\", train_recall)\n",
    "print(\"Testing recall:\", test_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7d658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This model is not suitable for our data or maybe the data needs more study and preproccessing\n",
    "## Data is: 1-Non-linearly separable data\n",
    "##          2-High-dimensional data\n",
    "##          3-Non-Gaussian distribution (LR assumes that data is gaussian)\n",
    "## all of these reasons may be the reason behind LR's poor performance "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
